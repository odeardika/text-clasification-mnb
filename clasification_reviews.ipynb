{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/eida/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerCaseAndNoNumber(list):\n",
    "    aftrerLower = []\n",
    "    for sentence in list :\n",
    "        noNumber = \"\"\n",
    "        for word in sentence:\n",
    "            if word.isdigit() == False :\n",
    "                noNumber += word\n",
    "        noNumber = noNumber.lower()\n",
    "        aftrerLower.append(noNumber)\n",
    "    return aftrerLower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuationRemover(list):\n",
    "    noPunctuation = []               \n",
    "    for i in list:\n",
    "        #^ = negasi  \n",
    "        #\\s = white space  \n",
    "        #\\w = word character\n",
    "        clean = re.sub(r\"[^\\w\\s]\", \"\", i) \n",
    "        noPunctuation.append(clean)\n",
    "    \n",
    "    return noPunctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojiRemover(list):\n",
    "    noEmoji = []\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "    for i in list:\n",
    "        clean = re.sub(emoj, \"\", i)\n",
    "        noEmoji.append(clean)\n",
    "    \n",
    "    return noEmoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(list):\n",
    "    normalText = []\n",
    "    for sentence in list:\n",
    "         #\\1 char pertama dari char yang diulang\n",
    "         #\\1+ char yang berulang\n",
    "        tempNormal =  re.sub(r'(\\w)\\1+', r'\\1', sentence) \n",
    "        normalText.append(tempNormal)\n",
    "    \n",
    "    return normalText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noAbbreviationAndTranslate(list):\n",
    "    abbrDictonary = {\n",
    "        'ohoho' : ' ','anganangan' : 'angan angan','oh' : ' ','ampunampunampun' : 'ampun ampun ampun','okdetak' : ' ','bang':'abang',\n",
    "        'skenarioeyobaru' : 'skenario baru','pun' : ' ','both' : 'keduannya','bose' : ' ','viva':' ','wade':' ','banal':' ','bands':'band','banga':' ','bangd':'abang',\n",
    "        'bothalchoractorschuminaroundgansumuminarumdrumstrumtrumminahumptadumpwaultopofoloderamaunsturnup' : ' ','botombay' : ' ','bapa':'bapak','baow':' ',\n",
    "        'botom' : 'bawah','botomia' : ' ','boy' : 'laki laki','brah' : ' ','brast' : ' ','brent' : ' ','brewinbaron' : ' ','brezed' : ' ','brezy' : ' ','briars' : ' ',\n",
    "        'bricks' : 'batu bata','abdon':' ','yulp':' ','zer':' ','zirkuvs':' ','zembliance':' ','nona':'wanita','alapala':' ','al':' ','aleconer':' ',\n",
    "        'bridget' : ' ','zastwoking':' ','acaras':' ','baik baik':'baik baik','bailey':' ','bakri':' ','balast':' ','band-band':'band band','baltxebec':' ',\n",
    "        'brightponthebaltic' : ' ','amamos':' ','ambulans':' ','wiliamson':' ','aleyeoneyesed':' ','anderson':' ',\n",
    "        'brinabath' : ' ','am':' ','badumdadi':' ','bae':' ','bagel':' ','wodnya':' ',\n",
    "        'bringback' : 'bawa kembali','wings':'sayap','with':'dengan','ugh':' ','uder':' ',\n",
    "        'brithyc' : ' ','anexandreian':' ','wish':'harapan','wo':' ','uhuh':' ','aum':' ',\n",
    "        'briz' : ' ','ap':'apa','apapa':'apa','ulstravoliance':' ','uh':' ','apopo':'',\n",
    "        'bro' : ' ','bad':'buruk','badumbadumdum':' ','secondniped': ' ','anexandreian':' ',\n",
    "        'brod' : ' ', 'clique':' ','ungkerl':' ','ukonen':' ','section':'bagian',\n",
    "        'a': ' ','u':' ','undangundang':'undang undang','blac':' ','black':'hitam',\n",
    "        'ababs' : ' ','used':'gunakan','tingi':' ','hun':' ','bothalchoractorschuminaroundgansumuminarumdrumstrumtrumminahumptadumpwaultopofoloderamaunsturnup':' ',\n",
    "        'abuabu' : 'abu abu','us':'kita','-':' ','blablablabla':' ','blablablablablabla':' ',\n",
    "        'tikustikus' : 'tikus tikus','':' ','berpurapura':'pura pura',\n",
    "        'mimpimimpiku' : 'mimpi mimpi','bidy':' ','acordial':'ramah',\n",
    "        'manusiamanusia' : 'manusia manusia','acros':'seberang','abrile':' ',\n",
    "        'jiwajiwa' : 'jiwa jiwa','utk':'untuk',\n",
    "        'you' : 'kamu','quiescence':' ','uzi':' ',\n",
    "        'might' : 'mungkin','v':' ','usung':' ',\n",
    "        'also' : 'juga',' ':' ',\n",
    "        'ohohoh' : ' ','rica':' ',\n",
    "        'bak' : ' ','rice':'nasi',\n",
    "        'hey' : 'raku',' ':' ','youngin':' ','yo':' ',\n",
    "        'racunracunracun' : 'racun racun racun',\n",
    "        'manusiayou' : 'manusia kamu',\n",
    "        'tejo' : ' ','wondernest':' ','wryghtly':' ',\n",
    "        'surti' : ' ','yah':' ','yamkoaronawa':' ','ye':' ','yid':' ',\n",
    "        'stack' : ' ','yahya':' ',\n",
    "        'ful' : ' ','yourself':'dirimu',\n",
    "        'time' : 'waktu','yous':' ',\n",
    "        'sat' : ' ','ysnod':' ',\n",
    "        'gwap' : ' ','youre':' ',\n",
    "        'untukmumumu' : 'untuk',\n",
    "        'wel' : ' ','adictiva':' ',\n",
    "        'lok' : ' ','\\n':' ',\n",
    "        'ford' : ' ','acau':' ',\n",
    "        'al' : 'semua',\n",
    "        'rockin' : ' ',\n",
    "        'nananana' : ' ',\n",
    "        'nanana' : ' ',\n",
    "        'ku' : ' ','andgloerdes':' ',\n",
    "        'styrmaly' : ' ',\n",
    "        'harihari' : 'hari',\n",
    "        'like dia' : 'seperti dia',\n",
    "        'aye' : ' ','and':'dan','anded':' ',\n",
    "        'es' : ' ',\n",
    "        'balin' : ' ',\n",
    "        'no' : 'tidak',\n",
    "        'history' : 'sejarah',\n",
    "        'like' : 'suka',\n",
    "        'adeyou' : 'kamu',\n",
    "        'awe' : ' ',\n",
    "        'yamko' : ' ',\n",
    "        'rambe' : ' ',\n",
    "        'legendayou' : ' ',\n",
    "        'ombe' : ' ',\n",
    "        'hei' : ' ',\n",
    "        'ngino' : ' ','after':'setelah',\n",
    "        'kibe' : ' ',\n",
    "        'kumbano' : ' ',\n",
    "        'kumbu' : ' ',\n",
    "        'beko' : ' ',\n",
    "        'yumano' : ' ',\n",
    "        'likeohoh' : ' ',\n",
    "        'ngak' : 'tidak',\n",
    "        'ho' : ' ','paw':' ','pay':'bayar',\n",
    "        'si' : ' ','agungsendirian':' ',\n",
    "        'cm' : 'cuma','ah':' ','aha' :' ','ahah' :' ','ahaha':' ',\n",
    "        'bkerja' : 'bekerja',\n",
    "        'o' : ' ',\n",
    "        'ya' : ' ',\n",
    "        'laki-laki' : 'laki laki',\n",
    "        'prety' : 'cantik',\n",
    "        'son' : 'putra',\n",
    "        'vs' : ' ',\n",
    "        'zma' : ' ',\n",
    "        'zo' : ' ',\n",
    "        'zot' : ' ',\n",
    "        'adalahaw' : ' ',\n",
    "        'ac' : ' ','sangupkah':'sanggup',\n",
    "        'yugi' : ' ',\n",
    "        'zegera' : ' ',\n",
    "        'zeke' : ' ',\n",
    "        'you' : 'kamu',\n",
    "        'your' : 'punya kamu',\n",
    "        'yeyeyeyeyeyeah' : ' ',\n",
    "        'yeaheyhey' : ' ',\n",
    "        'yes' : ' ',\n",
    "        'yg' : ' ',\n",
    "        'yanke' : ' ',\n",
    "        'yal' : ' ',\n",
    "        'xd' : ' ',\n",
    "        'x' : ' ','lovelines': ' ',\n",
    "        'wuke' : ' ',\n",
    "        'wk' : ' ',\n",
    "        'wkwkwk' : ' ',\n",
    "        'wit' : ' ','xtentacion':' ',\n",
    "        'wib' : ' ','y':' ',\n",
    "        'why' : ' ','wo':' ',\n",
    "        'wil' : 'akan','alsuficing':' ',\n",
    "        'will' : 'akan',\n",
    "        'wilpip' : ' ',\n",
    "        'warnawarni' : 'warna warna',\n",
    "        'warnawarna' : 'warna warna',\n",
    "        'waldemar' : ' ','waled':' ','walsh':' ','wakaka':' '\n",
    "    }\n",
    "    result = []\n",
    "    for sentence in list:\n",
    "        pattern =  re.compile(r'\\b('+'|'.join(abbrDictonary.keys())+r')\\b')        \n",
    "        temp = pattern.sub(lambda x: abbrDictonary[x.group()], sentence)\n",
    "        result.append(temp)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentence):\n",
    "    result = []\n",
    "    while(len(sentence) >= 5000):\n",
    "        i = 5000\n",
    "        while(sentence[i] != ' '):\n",
    "            i-=1\n",
    "        temp = sentence[:i]\n",
    "        print(temp)\n",
    "        result.append(temp)\n",
    "        sentence = sentence[i:]\n",
    "    result.append(sentence)\n",
    "    print(sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtranslate import translate\n",
    "def translate_to_indo(data):\n",
    "    result = []\n",
    "    for sentence in data:\n",
    "        if(len(sentence) >= 5000):\n",
    "            temp = ' '\n",
    "            list_sentence = split_sentence(sentence)\n",
    "            for i in list_sentence:\n",
    "               temp = temp + translate(i, 'id')\n",
    "        else:\n",
    "            temp = translate(sentence, 'id')\n",
    "        result.append(temp)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(list):\n",
    "    token = []\n",
    "    for sentence in list:\n",
    "        tempToken = []\n",
    "        for word in sentence.split():\n",
    "            tempToken.append(word)\n",
    "        token.append(tempToken)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWord(list):\n",
    "    stopwordIndo = set(stopwords.words('indonesian'))\n",
    "    \n",
    "    afterStopwords = []\n",
    "    for sentence in list :\n",
    "        tempFilter = []\n",
    "        for word in sentence:\n",
    "            if word not in stopwordIndo:\n",
    "                tempFilter.append(word)\n",
    "        afterStopwords.append(tempFilter)\n",
    "    \n",
    "    return afterStopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(list):\n",
    "    stemFactory = StemmerFactory()\n",
    "    nazief = stemFactory.create_stemmer('nazief')\n",
    "    \n",
    "    stem = []\n",
    "    for data in list:\n",
    "        tempStem = []\n",
    "        for word in data:\n",
    "            tempStem.append(nazief.stem(word))\n",
    "        stem.append(tempStem)\n",
    "        \n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSort(lists):\n",
    "    words = []\n",
    "    for Doc in lists:\n",
    "        # print(Doc)\n",
    "        for word in Doc:\n",
    "            # print(word)\n",
    "            if word not in words:\n",
    "                # print(type(word))\n",
    "                words.append(word)\n",
    "                # print(words)\n",
    "\n",
    "    words.sort()\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def TFIDF(list, words):\n",
    "    #banyak dokumen\n",
    "    #term frecuency\n",
    "    tf = []\n",
    "    for doc in list:\n",
    "        tempDoc = [1 for i in range(len(words))]\n",
    "        for word in doc:\n",
    "            tempDoc[words.index(word)] += 1\n",
    "        tf.append(tempDoc)\n",
    "    n = len(list) \n",
    "    df = [0 for i in range(len(words))]\n",
    "    for doc in list:\n",
    "        for word in set(doc):\n",
    "            if word in words:\n",
    "                index = words.index(word)\n",
    "                df[index] =+ 1\n",
    "    \n",
    "    idf = [0 for i in range(len(words))]\n",
    "    for i in range(len(df)):\n",
    "        if df[i] > 0:\n",
    "            idf[i] = math.log(n/df[i])\n",
    "    \n",
    "    tf_idf = []\n",
    "    for subList in range(len(tf)):\n",
    "        tempTFIDF = []\n",
    "        for i in words:\n",
    "            a = tf[subList][words.index(i)]\n",
    "            b = idf[words.index(i)]\n",
    "            tempTFIDF.append(a*b)\n",
    "        tf_idf.append(tempTFIDF)\n",
    "    \n",
    "    tf_idf = pd.DataFrame(tf_idf, index=[\"Dokumen\" + str(i+1) for i in range(len(tf_idf))])\n",
    "    for i in range(len(words)):\n",
    "        tf_idf = tf_idf.rename(columns={i:words[i]})\n",
    "    \n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_datas = pd.read_excel('reviews.xlsx')\n",
    "reviews = review_datas['Reviews']\n",
    "label_reviews = review_datas['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = lowerCaseAndNoNumber(reviews)\n",
    "reviews = emojiRemover(reviews)\n",
    "reviews = punctuationRemover(reviews)\n",
    "reviews = normalization(reviews)\n",
    "reviews = translate_to_indo(reviews)\n",
    "reviews = noAbbreviationAndTranslate(reviews)\n",
    "reviews = tokenization(reviews)\n",
    "reviews = stopWord(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_stem = stemming(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_word = wordSort(reviews_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_reviews = TFIDF(reviews_stem,reviews_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TFIDF_reviews\n",
    "y = label_reviews\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2 ,\n",
    "                                                    random_state=42)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_model_reviews.kmodel']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(model, 'train_model_reviews.kmodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8502994011976048\n",
      "Recall: 0.7590361445783133\n",
      "Precision: 0.9264705882352942\n",
      "F1-score: 0.8344370860927153\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy :\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"F1-score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
